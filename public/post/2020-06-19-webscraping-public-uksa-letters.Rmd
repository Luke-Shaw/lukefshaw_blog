---
title: Web scraping public UKSA letters
author: Luke Shaw
date: '2020-06-19'
output: 
  blogdown::html_page:
    toc: true
    number_sections: true
slug: webscraping-public-UKSA-letters
categories: []
tags: []
---
  
# Intro
  
The UK Statistics Authority (UKSA), and its regulatory arm the Office for Statistics Regulation (OSR), play the role of independently regulating and promoting good practice in [official statistics](https://www.statisticsauthority.gov.uk/about-the-authority/uk-statistical-system/types-of-official-statistics/). 

Part of this work is through written publicly available letters with producers of such statistics, the most high-profile of which was the [letter criticising the claim](https://www.statisticsauthority.gov.uk/wp-content/uploads/2017/09/Letter-from-Sir-David-Norgrove-to-Foreign-Secretary.pdf) of '£350 million per week' in relation to the UK leaving the EU.

I am interested in these letters, and having seen them in news stories recently I set out to see what data I could find.

# Scraping the site

I used the R package `rvest` to web scrape the data from their [website](https://www.statisticsauthority.gov.uk/correspondence-list/). 

This was quite fun, though fiddly at times. **All code is available in the GitHub repository [uksa-scrape](https://github.com/Luke-Shaw/uksa-scrape).** I learnt a bit about CSS and webpage construction on the way, and found this [tutorial](http://flukeout.github.io/) on CSS selectors fun and useful. The phrase 'scraping' feels appropriate, however, as it was not the smoothest of processes and involved a fair few try-and-see attempts until it looked about right!
  
I ended up with 2 datasets, one with the number of themed letters, and the other a list of all 1244 letters.

```{r read-data, warning=FALSE, echo=FALSE, message=FALSE, eval=TRUE}
library(tidyverse,quietly=TRUE)
library(knitr)
#read in data sets - could scrape live using code from https://github.com/Luke-Shaw/uksa-scrape
theme_tbl <- read_csv( "./2020-06-19-theme_tbl.csv")
letters_tbl <- read_csv( "./2020-06-19-letters_tbl.csv")
```

It is worth noting that the letters can be from official statistics creators to the UKSA as well as the other way round, so multiple letters can exist as a conversation between regulator and creator.

## Theme table and plot

Here are the first few elements of the theme table, which has 12 distinct themes and one total. Interestingly nearly half (46%) of the 1244 correspondences are not assigned a theme. This might be correct, or might be due to the theme being a recent addition to the 13 year-long data set.


```{r theme-tbl, echo=FALSE}
knitr::kable(head(theme_tbl %>% filter(values!=""),3))
```

From plotting the results we can see 'Health and Social Care' and 'Economy' are the themes with most letters.

```{r theme-plot, echo=FALSE, fig.width=10}
theme_plot <- ggplot(theme_tbl %>% filter(values!=""), 
                     aes(x = str_wrap(names,10), y = num_letters)) + 
  geom_bar(stat="identity") +
  theme_bw() + 
  labs(title = "Number of letters by theme",
       x = "Theme", 
       y = "Frequency",
       caption = "Source: https://www.statisticsauthority.gov.uk/correspondence-list/")
theme_plot
```

## Individual letters - 2020 in progress

I was thinking what interesting information I could try and glean from the individual letters. Here are the three most recent rows from the individual letter dataset that I scraped:

```{r letters-tbl, echo=FALSE}
knitr::kable(head(letters_tbl, 3))
```

Noticing that there have been 9 letters in June 2020 so far, including one today, I wondered if the number of letters was increasing year on year. Of course, as we haven't finished 2020, we need to adjust the other years to see if this year is on-track to be the year with the most letters.

```{r yearly-plot, echo=FALSE, fig.width=10}
# what day of the year is it right now (171 on 2020-06-19)
today_in_year <- lubridate::yday(Sys.Date())
# month and day for plot title
m <- lubridate::month(Sys.Date(), abbr=TRUE, label=TRUE)
d <- lubridate::day(Sys.Date()) %>% as.character()

# get the dataframe in format for plotting
yearly_correspondence <- letters_tbl %>%
  # find the year and whether it's before today in each year
mutate(year = lubridate::year(date),
       before_today_in_year = lubridate::yday(date) <= today_in_year) %>%
  mutate(before_today_in_year = ifelse(before_today_in_year,"Yes","No")) %>% 
  group_by(year, before_today_in_year) %>%
  count()

yearly_plot <- ggplot(yearly_correspondence, aes(x=year, y=n, fill=before_today_in_year)) +
  geom_bar(stat="identity") + 
  theme_bw() +
  scale_fill_manual(values = c("#17e33f","#007818")) +
  scale_x_continuous(breaks = seq(2008,2020,by=1)) +
  labs(title = paste0("UKSA correspondences",
                      "\nfilled by whether before ",
                      d, " ", m," in each year"),
       x = "Year",
       y = "Frequency",
       fill = "Before today \nin year",
       caption = paste0("Source: https://www.statisticsauthority.gov.uk/correspondence-list/",
                        "\nCode: https://github.com/Luke-Shaw/uksa-scrape"))
# annotate the plot - though will go out of date fairly quickly
yearly_plot + 
  annotate("rect", xmin = 2017, xmax = 2021, ymin = 60, ymax = 120, alpha = .2) +
  annotate("text", x=2020.5, y = 145, 
           label = str_wrap("2020 set to be highest, but similar to last 2 years",
                            width=15))

```


So the answer is **yes**, although this year is looking similar to the last two.

# Further analysis

This is quite a fun data set, and there is more that could be done with it. It might be interesting to try natural language processing (NLP) on the information in the titles or entire letter text, or to see how frequently different government departments are involved. A way of identifying whether it was an inbound or outbound letter, which may well be hidden in the webpage somewhere, would be an interesting additional column to the letters dataset.

As mentioned above, all the code is on [GitHub](https://github.com/Luke-Shaw/uksa-scrape) if anyone out there wants to play around further.

# Conclusion

The work the UKSA does is vital in promoting and safeguarding the production and publication of official statistics that ‘serve the public good’. OK, I took most of that last sentence from their website, but I do believe it.

Through scraping publicly available information, we can see that the UKSA is on track to publish the most letters this year, though the volume is similar to 2018 and 2019.
